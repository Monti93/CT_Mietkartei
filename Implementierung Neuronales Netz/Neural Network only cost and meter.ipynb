{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 2  # used for shaping training and test input data\n",
    "number_of_labels = 2    # used for shaping the one-hot encoded label for predicted classes\n",
    "batch_size = 100        # used for batch learning\n",
    "generations=5000        # number of generations used for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that transforms all monthly costs into a number\n",
    "def transformCosts(rentalFee, additionalCosts):\n",
    "    fee      = re.search('([0-9]*)-([0-9]*)', rentalFee) # extract the two numbers in range of rentalFee i.e. \"251-300\" ==> 251 and 300\n",
    "    addCosts = re.search('(unter |ueber |[0-9]*)-?([0-9]*)', additionalCosts) # extract the two numbers or the single number of additional costs i.e. \"50-100\" ==> 50 and 100 or \"unter 50\" ==> 50\n",
    "    \n",
    "    # compute the middle between those two numbers or simply take the only given number if only one exists\n",
    "    midFee = (int(fee.group(1)) + int(fee.group(2))) / 2\n",
    "    try:\n",
    "        # works if both groups are numbers\n",
    "        midAddCosts = (int(addCosts.group(1)) + int(addCosts.group(2))) / 2\n",
    "    except ValueError:\n",
    "        # happens if first group is no number\n",
    "        midAddCosts = int(addCosts.group(2))\n",
    "    return midFee + midAddCosts\n",
    "\n",
    "# helper function that transforms the range of square meters to the mid number\n",
    "def transformSquareMeter(squareMeter):\n",
    "    # works similar to additional cost extraction in transformCosts()\n",
    "    sm = re.search('(bis |ueber |[0-9]*)-?([0-9]*)', squareMeter)\n",
    "    try:\n",
    "        return (int(sm.group(1)) + int(sm.group(2))) / 2\n",
    "    except ValueError:\n",
    "        return int(sm.group(2))\n",
    "    \n",
    "# helper function that transforms the classes \"ja\" and \"nein\" to one-hot encoded array \n",
    "def transformPrediction(target):\n",
    "    if(target == \"ja\"):\n",
    "        return np.array([0,1])\n",
    "    else:\n",
    "        return np.array([1,0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to perform min-max scaling of features\n",
    "def scaleFeatures(features):\n",
    "    min = features.min(0) # get min values for every feature column-wise\n",
    "    max = features.max(0) # get max values for every feature column-wise\n",
    "    for row in range(features.shape[0]):\n",
    "        for attr in range(min.shape[0]):\n",
    "            features[row, attr] = (features[row, attr] - min[attr]) / (max[attr] - min[attr]) # scale each feature based on min-max normalization\n",
    "    return features\n",
    "\n",
    "def transformCsvToFeatures(csvUrl):\n",
    "    with open(csvUrl, 'rt') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        next(reader, None) # skip the header\n",
    "        X = np.array([]) # matrix for features\n",
    "        Y = np.array([]) # one-hot encoded matrix for labels (classes)\n",
    "        rowCount = 0\n",
    "        for row in reader:\n",
    "            #print(row[8], row[9], row[21], row[22])\n",
    "            X = np.append(X, np.array([transformCosts(row[8], row[9]), transformSquareMeter(row[21])]))\n",
    "            Y = np.append(Y, transformPrediction(row[22]))\n",
    "            rowCount += 1\n",
    "        X.shape = (rowCount, number_of_features)\n",
    "        Y.shape = (rowCount, number_of_labels)\n",
    "        return (scaleFeatures(X), Y) \n",
    "        #print(X)\n",
    "        #print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Features = transformCsvToFeatures('Wohnungskartei_Muster_Master_4_S.csv')\n",
    "test_Features = transformCsvToFeatures('Wohnungskartei_Muster_Master_6_S_teach.csv')\n",
    "# features can be accessed via index 0, labels via index 1\n",
    "# i.e. train_Features[0] --> train features\n",
    "# i.e. train_Features[1] --> train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_first_layer  = 100 # number of neurons used in first hidden layer \n",
    "neurons_second_layer = 50  # number of neurons used in second hidden layer\n",
    "neurons_third_layer  = 25  # number of neurons used in third hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, number_of_features])  # Tensoflow placeholder for training samples\n",
    "Y_ = tf.placeholder(tf.float32, [None, number_of_labels])   # Tensoflow placeholder for training labels (the provided ones as teacher)\n",
    "learning_rate = tf.placeholder(tf.float32)                  # Tensoflow placeholder for learning rate controls how big adjustment steps for backpropagation should be\n",
    "prob_keep = tf.placeholder(tf.float32)                      # Tensoflow placeholder for the propability for neurons to be kept in each layer (dropout) to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights will be initialized with small random values between -0.2 and +0.2\n",
    "# RELU will be used as activation function, it is common then to initialize biases with small positive values for example 0.1 = tf.ones([...])/10\n",
    "\n",
    "# weights and biases from input to first hidden layer\n",
    "W1 = tf.Variable(tf.truncated_normal([number_of_features, neurons_first_layer], stddev=0.1), name=\"W1\")  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.ones([neurons_first_layer])/10, name=\"B1\")\n",
    "\n",
    "# weights and biases from first hidden to second hidden layer\n",
    "W2 = tf.Variable(tf.truncated_normal([neurons_first_layer, neurons_second_layer], stddev=0.1), name=\"W2\")\n",
    "B2 = tf.Variable(tf.ones([neurons_second_layer])/10, name=\"B2\")\n",
    "\n",
    "# weights and biases from second hidden to third hidden layer\n",
    "W3 = tf.Variable(tf.truncated_normal([neurons_second_layer, neurons_third_layer], stddev=0.1), name=\"W3\")\n",
    "B3 = tf.Variable(tf.ones([neurons_third_layer])/10, name=\"B3\")\n",
    "\n",
    "# weights and biases from third hidden to output layer\n",
    "W4 = tf.Variable(tf.truncated_normal([neurons_third_layer, number_of_labels], stddev=0.1), name=\"W4\")\n",
    "B4 = tf.Variable(tf.ones([number_of_labels])/10, name=\"B4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model output with dropout at each layer\n",
    "# RELU as activation function\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, prob_keep)\n",
    "\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "Y2d = tf.nn.dropout(Y2, prob_keep)\n",
    "\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "Y3d = tf.nn.dropout(Y3, prob_keep)\n",
    "\n",
    "# last layer with softmax to get continuous values between 0 an 1\n",
    "Ylogits = tf.matmul(Y3d, W4) + B4\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# argmax returns the highest value representing the most propable class regarding the one-hot encoded labeling (used for model output later on)\n",
    "Y_one_hot = tf.argmax(Y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy will be used as loss function ==> -sum(Y_i * log(Yi)) \n",
    "# the value will be normalised for batches of batch_size entries as initialized in the beginning\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of current model between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function can be called in a loop to train the model and store evaluation results\n",
    "def training_step(i, update_test_data, update_train_data, last_step):\n",
    "    rand_index = np.random.choice(len(train_Features[0]), size=batch_size) # returns random indizes with shape of batch size\n",
    "    batch_X = train_Features[0][rand_index] # select features based on random indices\n",
    "    batch_Y = train_Features[1][rand_index] # select labels based of random indices\n",
    "    \n",
    "    # adjust the learning rate based on learning progress --> it becomes smaller each generation\n",
    "    max_learning_rate = 0.003\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "    decayed_learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    \n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, prob_keep: 0.75, learning_rate: decayed_learning_rate})\n",
    "\n",
    "    a, c = sess.run([accuracy, cross_entropy], {X: batch_X, Y_: batch_Y, prob_keep: 1.0})\n",
    "\n",
    "    if update_train_data:\n",
    "        # print training evaluation values\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (learning rate:\" + str(decayed_learning_rate) + \")\")\n",
    "\n",
    "    if update_test_data:\n",
    "        # print test evaluation values\n",
    "        train_loss.append(c)\n",
    "        train_acc.append(a)\n",
    "        a, c = sess.run([accuracy, cross_entropy], {X: test_Features[0], Y_: test_Features[1], prob_keep: 1.0})\n",
    "        test_acc.append(a)\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//test_Features[0].shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.95 loss: 64.11442 (learning rate:0.003)\n",
      "0: ********* epoch 1 ********* test accuracy:0.9260274 test loss: 64.32807\n",
      "20: accuracy:0.99 loss: 4.882611 (learning rate:0.0029711445178725875)\n",
      "40: accuracy:0.93 loss: 21.003502 (learning rate:0.0029425761525895904)\n",
      "60: accuracy:0.97 loss: 9.45515 (learning rate:0.0029142920472906737)\n",
      "80: accuracy:0.98 loss: 6.430329 (learning rate:0.0028862893735417373)\n",
      "100: accuracy:0.98 loss: 5.416449 (learning rate:0.0028585653310520707)\n",
      "100: ********* epoch 28 ********* test accuracy:0.9260274 test loss: 13.589564\n",
      "120: accuracy:0.98 loss: 4.672941 (learning rate:0.0028311171473943213)\n",
      "140: accuracy:0.99 loss: 2.0654268 (learning rate:0.0028039420777272502)\n",
      "160: accuracy:0.97 loss: 6.410644 (learning rate:0.0027770374045212438)\n",
      "180: accuracy:0.95 loss: 6.54581 (learning rate:0.0027504004372865616)\n",
      "200: accuracy:0.92 loss: 10.835337 (learning rate:0.0027240285123042826)\n",
      "200: ********* epoch 55 ********* test accuracy:0.9260274 test loss: 9.9366455\n",
      "220: accuracy:0.97 loss: 5.337792 (learning rate:0.0026979189923599317)\n",
      "240: accuracy:0.93 loss: 9.278558 (learning rate:0.0026720692664797567)\n",
      "260: accuracy:0.97 loss: 6.400475 (learning rate:0.0026464767496696276)\n",
      "280: accuracy:0.99 loss: 3.6782312 (learning rate:0.002621138882656537)\n",
      "300: accuracy:0.99 loss: 2.4549897 (learning rate:0.0025960531316326675)\n",
      "300: ********* epoch 83 ********* test accuracy:0.9260274 test loss: 8.180664\n",
      "320: accuracy:1.0 loss: 2.5056913 (learning rate:0.002571216988002013)\n",
      "340: accuracy:0.99 loss: 3.1744852 (learning rate:0.002546627968129513)\n",
      "360: accuracy:1.0 loss: 3.297354 (learning rate:0.0025222836130926888)\n",
      "380: accuracy:0.98 loss: 4.339023 (learning rate:0.0024981814884357505)\n",
      "400: accuracy:1.0 loss: 1.4549599 (learning rate:0.0024743191839261473)\n",
      "400: ********* epoch 110 ********* test accuracy:0.9808219 test loss: 5.1275873\n",
      "420: accuracy:1.0 loss: 1.9268867 (learning rate:0.0024506943133135424)\n",
      "440: accuracy:0.99 loss: 2.5867105 (learning rate:0.0024273045140911875)\n",
      "460: accuracy:1.0 loss: 1.047121 (learning rate:0.0024041474472596687)\n",
      "480: accuracy:0.99 loss: 2.726355 (learning rate:0.002381220797093005)\n",
      "500: accuracy:1.0 loss: 0.6390686 (learning rate:0.002358522270907074)\n",
      "500: ********* epoch 137 ********* test accuracy:0.9780822 test loss: 3.2989984\n",
      "520: accuracy:1.0 loss: 1.5111417 (learning rate:0.0023360495988303423)\n",
      "540: accuracy:0.99 loss: 2.8148086 (learning rate:0.002313800533576874)\n",
      "560: accuracy:1.0 loss: 0.63749623 (learning rate:0.0022917728502216037)\n",
      "580: accuracy:1.0 loss: 0.17762193 (learning rate:0.0022699643459778394)\n",
      "600: accuracy:0.99 loss: 3.11106 (learning rate:0.002248372839976982)\n",
      "600: ********* epoch 165 ********* test accuracy:0.9972603 test loss: 1.2177441\n",
      "620: accuracy:1.0 loss: 0.11974898 (learning rate:0.0022269961730504387)\n",
      "640: accuracy:1.0 loss: 0.09653675 (learning rate:0.0022058322075137037)\n",
      "660: accuracy:1.0 loss: 0.19353041 (learning rate:0.0021848788269525857)\n",
      "680: accuracy:0.99 loss: 3.1262093 (learning rate:0.002164133936011568)\n",
      "700: accuracy:1.0 loss: 0.20117788 (learning rate:0.002143595460184269)\n",
      "700: ********* epoch 192 ********* test accuracy:0.9972603 test loss: 1.1437273\n",
      "720: accuracy:1.0 loss: 0.12965423 (learning rate:0.00212326134560599)\n",
      "740: accuracy:1.0 loss: 0.11825382 (learning rate:0.0021031295588483287)\n",
      "760: accuracy:1.0 loss: 0.17919372 (learning rate:0.002083198086715832)\n",
      "780: accuracy:1.0 loss: 0.10093149 (learning rate:0.0020634649360446776)\n",
      "800: accuracy:1.0 loss: 0.1021325 (learning rate:0.002043928133503354)\n",
      "800: ********* epoch 220 ********* test accuracy:0.9972603 test loss: 1.1478554\n",
      "820: accuracy:1.0 loss: 0.13801858 (learning rate:0.0020245857253953265)\n",
      "840: accuracy:0.99 loss: 3.6424358 (learning rate:0.0020054357774636645)\n",
      "860: accuracy:1.0 loss: 0.09867723 (learning rate:0.001986476374697618)\n",
      "880: accuracy:1.0 loss: 0.03587909 (learning rate:0.00196770562114111)\n",
      "900: accuracy:1.0 loss: 0.07020319 (learning rate:0.001949121639703143)\n",
      "900: ********* epoch 247 ********* test accuracy:0.9972603 test loss: 1.0164852\n",
      "920: accuracy:1.0 loss: 0.048186176 (learning rate:0.0019307225719700854)\n",
      "940: accuracy:1.0 loss: 0.22696003 (learning rate:0.0019125065780198325)\n",
      "960: accuracy:1.0 loss: 0.06694012 (learning rate:0.0018944718362378086)\n",
      "980: accuracy:1.0 loss: 0.08267526 (learning rate:0.0018766165431348069)\n",
      "1000: accuracy:1.0 loss: 0.15068752 (learning rate:0.0018589389131666372)\n",
      "1000: ********* epoch 274 ********* test accuracy:0.9972603 test loss: 1.4412085\n",
      "1020: accuracy:0.99 loss: 2.9241328 (learning rate:0.0018414371785555712)\n",
      "1040: accuracy:1.0 loss: 0.264384 (learning rate:0.001824109589113564)\n",
      "1060: accuracy:1.0 loss: 0.088092044 (learning rate:0.0018069544120672303)\n",
      "1080: accuracy:1.0 loss: 0.04101343 (learning rate:0.0017899699318845701)\n",
      "1100: accuracy:1.0 loss: 0.14911422 (learning rate:0.0017731544501034114)\n",
      "1100: ********* epoch 302 ********* test accuracy:0.9972603 test loss: 1.2856692\n",
      "1120: accuracy:1.0 loss: 0.11683339 (learning rate:0.0017565062851615633)\n",
      "1140: accuracy:1.0 loss: 0.093557075 (learning rate:0.0017400237722286578)\n",
      "1160: accuracy:1.0 loss: 0.118706144 (learning rate:0.001723705263039666)\n",
      "1180: accuracy:1.0 loss: 0.0976089 (learning rate:0.0017075491257300707)\n",
      "1200: accuracy:1.0 loss: 0.07799766 (learning rate:0.001691553744672677)\n",
      "1200: ********* epoch 329 ********* test accuracy:0.9972603 test loss: 1.276793\n",
      "1220: accuracy:1.0 loss: 0.062815435 (learning rate:0.0016757175203160495)\n",
      "1240: accuracy:0.99 loss: 3.7712936 (learning rate:0.001660038869024556)\n",
      "1260: accuracy:1.0 loss: 0.10059852 (learning rate:0.001644516222920002)\n",
      "1280: accuracy:0.99 loss: 3.3540652 (learning rate:0.001629148029724841)\n",
      "1300: accuracy:1.0 loss: 0.20393503 (learning rate:0.0016139327526069466)\n",
      "1300: ********* epoch 357 ********* test accuracy:0.9972603 test loss: 1.4053193\n",
      "1320: accuracy:1.0 loss: 0.025099568 (learning rate:0.0015988688700259279)\n",
      "1340: accuracy:1.0 loss: 0.011274935 (learning rate:0.0015839548755809732)\n",
      "1360: accuracy:1.0 loss: 0.073768094 (learning rate:0.0015691892778602098)\n",
      "1380: accuracy:1.0 loss: 0.08207717 (learning rate:0.0015545706002915614)\n",
      "1400: accuracy:1.0 loss: 0.088871 (learning rate:0.0015400973809950877)\n",
      "1400: ********* epoch 384 ********* test accuracy:0.9972603 test loss: 1.4419738\n",
      "1420: accuracy:1.0 loss: 0.03746109 (learning rate:0.001525768172636799)\n",
      "1440: accuracy:1.0 loss: 0.15191838 (learning rate:0.001511581542283918)\n",
      "1460: accuracy:1.0 loss: 0.07649214 (learning rate:0.0014975360712615872)\n",
      "1480: accuracy:1.0 loss: 0.07724801 (learning rate:0.0014836303550109999)\n",
      "1500: accuracy:1.0 loss: 0.11743067 (learning rate:0.0014698630029489428)\n",
      "1500: ********* epoch 411 ********* test accuracy:0.9972603 test loss: 1.3873671\n",
      "1520: accuracy:1.0 loss: 0.11590853 (learning rate:0.0014562326383287369)\n",
      "1540: accuracy:1.0 loss: 0.3337295 (learning rate:0.0014427378981025616)\n",
      "1560: accuracy:1.0 loss: 0.060246214 (learning rate:0.0014293774327851483)\n",
      "1580: accuracy:1.0 loss: 0.13191785 (learning rate:0.001416149906318832)\n",
      "1600: accuracy:1.0 loss: 0.028420344 (learning rate:0.0014030539959399427)\n",
      "1600: ********* epoch 439 ********* test accuracy:0.9972603 test loss: 1.3947133\n",
      "1620: accuracy:0.99 loss: 3.4306474 (learning rate:0.0013900883920465294)\n",
      "1640: accuracy:1.0 loss: 0.1517114 (learning rate:0.001377251798067398)\n",
      "1660: accuracy:0.99 loss: 3.0030246 (learning rate:0.0013645429303324535)\n",
      "1680: accuracy:1.0 loss: 0.08996943 (learning rate:0.0013519605179443314)\n",
      "1700: accuracy:1.0 loss: 0.13825545 (learning rate:0.0013395033026513076)\n",
      "1700: ********* epoch 466 ********* test accuracy:0.9972603 test loss: 1.4518551\n",
      "1720: accuracy:1.0 loss: 0.112657234 (learning rate:0.0013271700387214717)\n",
      "1740: accuracy:1.0 loss: 0.048787735 (learning rate:0.0013149594928181531)\n",
      "1760: accuracy:1.0 loss: 0.14574474 (learning rate:0.0013028704438765861)\n",
      "1780: accuracy:1.0 loss: 0.11824916 (learning rate:0.001290901682981802)\n",
      "1800: accuracy:1.0 loss: 0.02748574 (learning rate:0.0012790520132477375)\n",
      "1800: ********* epoch 494 ********* test accuracy:0.9972603 test loss: 1.4507674\n",
      "1820: accuracy:1.0 loss: 0.054918766 (learning rate:0.0012673202496975445)\n",
      "1840: accuracy:1.0 loss: 0.039537534 (learning rate:0.0012557052191450912)\n",
      "1860: accuracy:1.0 loss: 0.051764697 (learning rate:0.0012442057600776434)\n",
      "1880: accuracy:1.0 loss: 0.14128853 (learning rate:0.0012328207225397114)\n",
      "1900: accuracy:1.0 loss: 0.08741006 (learning rate:0.0012215489680180538)\n",
      "1900: ********* epoch 521 ********* test accuracy:0.9972603 test loss: 1.4888225\n",
      "1920: accuracy:0.99 loss: 3.4453168 (learning rate:0.0012103893693278251)\n",
      "1940: accuracy:1.0 loss: 0.15501378 (learning rate:0.0011993408104998568)\n",
      "1960: accuracy:1.0 loss: 0.12026305 (learning rate:0.001188402186669059)\n",
      "1980: accuracy:1.0 loss: 0.15917204 (learning rate:0.0011775724039639326)\n",
      "2000: accuracy:1.0 loss: 0.17188755 (learning rate:0.0011668503793971828)\n",
      "2000: ********* epoch 548 ********* test accuracy:0.9972603 test loss: 1.5006838\n",
      "2020: accuracy:1.0 loss: 0.043180935 (learning rate:0.0011562350407574179)\n",
      "2040: accuracy:1.0 loss: 0.007936766 (learning rate:0.0011457253265019273)\n",
      "2060: accuracy:1.0 loss: 0.020947471 (learning rate:0.0011353201856505275)\n",
      "2080: accuracy:1.0 loss: 0.109935924 (learning rate:0.0011250185776804627)\n",
      "2100: accuracy:1.0 loss: 0.06615797 (learning rate:0.0011148194724223506)\n",
      "2100: ********* epoch 576 ********* test accuracy:0.9972603 test loss: 1.5453669\n",
      "2120: accuracy:1.0 loss: 0.03078348 (learning rate:0.0011047218499571666)\n",
      "2140: accuracy:1.0 loss: 0.09203779 (learning rate:0.0010947247005142493)\n",
      "2160: accuracy:1.0 loss: 0.012793853 (learning rate:0.0010848270243703235)\n",
      "2180: accuracy:1.0 loss: 0.06816907 (learning rate:0.0010750278317495268)\n",
      "2200: accuracy:1.0 loss: 0.20044588 (learning rate:0.0010653261427244307)\n",
      "2200: ********* epoch 603 ********* test accuracy:0.9972603 test loss: 1.4470866\n",
      "2220: accuracy:1.0 loss: 0.0066837836 (learning rate:0.0010557209871180483)\n",
      "2240: accuracy:1.0 loss: 0.060406223 (learning rate:0.0010462114044068145)\n",
      "2260: accuracy:1.0 loss: 0.001234286 (learning rate:0.0010367964436245336)\n",
      "2280: accuracy:1.0 loss: 0.051424112 (learning rate:0.0010274751632672816)\n",
      "2300: accuracy:1.0 loss: 0.035839837 (learning rate:0.0010182466311992545)\n",
      "2300: ********* epoch 631 ********* test accuracy:0.9972603 test loss: 1.5106658\n",
      "2320: accuracy:0.99 loss: 3.6432586 (learning rate:0.0010091099245595554)\n",
      "2340: accuracy:1.0 loss: 0.018994845 (learning rate:0.0010000641296699067)\n",
      "2360: accuracy:1.0 loss: 0.122661315 (learning rate:0.0009911083419432806)\n",
      "2380: accuracy:1.0 loss: 0.07791726 (learning rate:0.0009822416657934419)\n",
      "2400: accuracy:1.0 loss: 0.09410307 (learning rate:0.0009734632145453863)\n",
      "2400: ********* epoch 658 ********* test accuracy:0.9972603 test loss: 1.5561503\n",
      "2420: accuracy:1.0 loss: 0.15447626 (learning rate:0.0009647721103466736)\n",
      "2440: accuracy:1.0 loss: 0.08698341 (learning rate:0.0009561674840796413)\n",
      "2460: accuracy:1.0 loss: 0.12006745 (learning rate:0.0009476484752744924)\n",
      "2480: accuracy:1.0 loss: 0.04339639 (learning rate:0.0009392142320232469)\n",
      "2500: accuracy:1.0 loss: 0.047490615 (learning rate:0.0009308639108945514)\n",
      "2500: ********* epoch 685 ********* test accuracy:0.9972603 test loss: 1.5187786\n",
      "2520: accuracy:1.0 loss: 0.09599271 (learning rate:0.0009225966768493343)\n",
      "2540: accuracy:1.0 loss: 0.07998011 (learning rate:0.0009144117031573014)\n",
      "2560: accuracy:1.0 loss: 0.056826994 (learning rate:0.0009063081713142631)\n",
      "2580: accuracy:1.0 loss: 0.040853273 (learning rate:0.0008982852709602818)\n",
      "2600: accuracy:1.0 loss: 0.009658848 (learning rate:0.0008903421997986366)\n",
      "2600: ********* epoch 713 ********* test accuracy:0.9972603 test loss: 1.3385109\n",
      "2620: accuracy:1.0 loss: 0.11138556 (learning rate:0.0008824781635155918)\n",
      "2640: accuracy:1.0 loss: 0.057449855 (learning rate:0.000874692375700966)\n",
      "2660: accuracy:1.0 loss: 0.08246997 (learning rate:0.0008669840577694896)\n",
      "2680: accuracy:1.0 loss: 0.084625155 (learning rate:0.0008593524388829455)\n",
      "2700: accuracy:1.0 loss: 0.023093848 (learning rate:0.0008517967558730855)\n",
      "2700: ********* epoch 740 ********* test accuracy:0.9972603 test loss: 1.5406605\n",
      "2720: accuracy:1.0 loss: 0.07704546 (learning rate:0.0008443162531653122)\n",
      "2740: accuracy:1.0 loss: 0.025335213 (learning rate:0.0008369101827031209)\n",
      "2760: accuracy:1.0 loss: 0.02574304 (learning rate:0.000829577803873294)\n",
      "2780: accuracy:1.0 loss: 0.021005705 (learning rate:0.000822318383431838)\n",
      "2800: accuracy:1.0 loss: 0.10785576 (learning rate:0.0008151311954306589)\n",
      "2800: ********* epoch 768 ********* test accuracy:0.9972603 test loss: 1.5321004\n",
      "2820: accuracy:1.0 loss: 0.0430959 (learning rate:0.0008080155211449677)\n",
      "2840: accuracy:1.0 loss: 0.020986747 (learning rate:0.0008009706490014058)\n",
      "2860: accuracy:1.0 loss: 0.053655002 (learning rate:0.0007939958745068883)\n",
      "2880: accuracy:1.0 loss: 0.0003821694 (learning rate:0.0007870905001781532)\n",
      "2900: accuracy:1.0 loss: 0.008939325 (learning rate:0.0007802538354720133)\n",
      "2900: ********* epoch 795 ********* test accuracy:0.9972603 test loss: 1.5478175\n",
      "2920: accuracy:0.99 loss: 3.2670255 (learning rate:0.0007734851967163007)\n",
      "2940: accuracy:0.98 loss: 6.1643076 (learning rate:0.0007667839070414992)\n",
      "2960: accuracy:1.0 loss: 0.34874266 (learning rate:0.000760149296313057)\n",
      "2980: accuracy:1.0 loss: 0.17772888 (learning rate:0.0007535807010643724)\n",
      "3000: accuracy:1.0 loss: 0.13330162 (learning rate:0.0007470774644304465)\n",
      "3000: ********* epoch 822 ********* test accuracy:0.9972603 test loss: 1.5563782\n",
      "3020: accuracy:1.0 loss: 0.14395663 (learning rate:0.0007406389360821968)\n",
      "3040: accuracy:0.99 loss: 3.3352234 (learning rate:0.0007342644721614229)\n",
      "3060: accuracy:1.0 loss: 0.047571253 (learning rate:0.0007279534352164206)\n",
      "3080: accuracy:1.0 loss: 0.039754808 (learning rate:0.0007217051941382361)\n",
      "3100: accuracy:1.0 loss: 0.045530647 (learning rate:0.0007155191240975549)\n",
      "3100: ********* epoch 850 ********* test accuracy:0.9972603 test loss: 1.0309856\n",
      "3120: accuracy:1.0 loss: 0.123261824 (learning rate:0.0007093946064822178)\n",
      "3140: accuracy:1.0 loss: 0.035553303 (learning rate:0.0007033310288353594)\n",
      "3160: accuracy:1.0 loss: 0.046602406 (learning rate:0.000697327784794162)\n",
      "3180: accuracy:1.0 loss: 0.04376201 (learning rate:0.000691384274029219)\n",
      "3200: accuracy:1.0 loss: 0.01822781 (learning rate:0.0006854999021845007)\n",
      "3200: ********* epoch 877 ********* test accuracy:0.9972603 test loss: 1.5577446\n",
      "3220: accuracy:1.0 loss: 0.06672463 (learning rate:0.0006796740808179191)\n",
      "3240: accuracy:1.0 loss: 0.10332294 (learning rate:0.0006739062273424826)\n",
      "3260: accuracy:1.0 loss: 0.08051047 (learning rate:0.0006681957649680373)\n",
      "3280: accuracy:1.0 loss: 0.13804774 (learning rate:0.0006625421226435866)\n",
      "3300: accuracy:1.0 loss: 0.06172417 (learning rate:0.000656944735000187)\n",
      "3300: ********* epoch 905 ********* test accuracy:0.9972603 test loss: 1.5434761\n",
      "3320: accuracy:1.0 loss: 0.05088521 (learning rate:0.0006514030422944097)\n",
      "3340: accuracy:1.0 loss: 0.14486603 (learning rate:0.0006459164903523658)\n",
      "3360: accuracy:1.0 loss: 0.050983958 (learning rate:0.000640484530514289)\n",
      "3380: accuracy:1.0 loss: 0.08959323 (learning rate:0.000635106619579669)\n",
      "3400: accuracy:1.0 loss: 0.12842926 (learning rate:0.0006297822197529307)\n",
      "3400: ********* epoch 932 ********* test accuracy:0.9972603 test loss: 1.5594516\n",
      "3420: accuracy:1.0 loss: 0.020438507 (learning rate:0.0006245107985896541)\n",
      "3440: accuracy:1.0 loss: 0.08241776 (learning rate:0.0006192918289433304)\n",
      "3460: accuracy:1.0 loss: 0.06352566 (learning rate:0.0006141247889126458)\n",
      "3480: accuracy:1.0 loss: 0.10755311 (learning rate:0.000609009161789291)\n",
      "3500: accuracy:1.0 loss: 0.0054735932 (learning rate:0.000603944436006291)\n",
      "3500: ********* epoch 959 ********* test accuracy:0.9972603 test loss: 1.567992\n",
      "3520: accuracy:1.0 loss: 0.084194966 (learning rate:0.0005989301050868467)\n",
      "3540: accuracy:1.0 loss: 0.07558232 (learning rate:0.0005939656675936874)\n",
      "3560: accuracy:1.0 loss: 0.067130245 (learning rate:0.000589050627078927)\n",
      "3580: accuracy:1.0 loss: 0.2158718 (learning rate:0.000584184492034418)\n",
      "3600: accuracy:1.0 loss: 0.049341068 (learning rate:0.000579366775842601)\n",
      "3600: ********* epoch 987 ********* test accuracy:0.9972603 test loss: 1.5916433\n",
      "3620: accuracy:1.0 loss: 0.0905073 (learning rate:0.0005745969967278418)\n",
      "3640: accuracy:1.0 loss: 0.0053806957 (learning rate:0.0005698746777082543)\n",
      "3660: accuracy:0.98 loss: 6.672892 (learning rate:0.000565199346548001)\n",
      "3680: accuracy:1.0 loss: 0.052060604 (learning rate:0.00056057053571007)\n",
      "3700: accuracy:1.0 loss: 0.17678168 (learning rate:0.0005559877823095201)\n",
      "3700: ********* epoch 1014 ********* test accuracy:0.9972603 test loss: 1.5978237\n",
      "3720: accuracy:1.0 loss: 0.12941304 (learning rate:0.0005514506280671922)\n",
      "3740: accuracy:1.0 loss: 0.093243234 (learning rate:0.0005469586192638811)\n",
      "3760: accuracy:1.0 loss: 0.057099674 (learning rate:0.0005425113066949633)\n",
      "3780: accuracy:1.0 loss: 0.04979969 (learning rate:0.0005381082456254755)\n",
      "3800: accuracy:1.0 loss: 0.10014957 (learning rate:0.0005337489957456418)\n",
      "3800: ********* epoch 1042 ********* test accuracy:0.9972603 test loss: 1.6371615\n",
      "3820: accuracy:1.0 loss: 0.16077897 (learning rate:0.0005294331211268412)\n",
      "3840: accuracy:1.0 loss: 0.035416186 (learning rate:0.0005251601901780155)\n",
      "3860: accuracy:1.0 loss: 0.03824028 (learning rate:0.0005209297756025089)\n",
      "3880: accuracy:1.0 loss: 0.066043206 (learning rate:0.0005167414543553385)\n",
      "3900: accuracy:1.0 loss: 0.13318703 (learning rate:0.0005125948076008895)\n",
      "3900: ********* epoch 1069 ********* test accuracy:0.9972603 test loss: 1.6537033\n",
      "3920: accuracy:1.0 loss: 0.040205467 (learning rate:0.0005084894206710306)\n",
      "3940: accuracy:1.0 loss: 0.074372746 (learning rate:0.0005044248830236478)\n",
      "3960: accuracy:0.99 loss: 3.4494286 (learning rate:0.0005004007882015892)\n",
      "3980: accuracy:1.0 loss: 0.017119695 (learning rate:0.0004964167337920192)\n",
      "4000: accuracy:1.0 loss: 0.06769328 (learning rate:0.0004924723213861769)\n",
      "4000: ********* epoch 1096 ********* test accuracy:0.9972603 test loss: 1.6717613\n",
      "4020: accuracy:1.0 loss: 0.0073384903 (learning rate:0.0004885671565395345)\n",
      "4040: accuracy:1.0 loss: 0.15673938 (learning rate:0.00048470084873235297)\n",
      "4060: accuracy:1.0 loss: 0.0095102275 (learning rate:0.00048087301133063005)\n",
      "4080: accuracy:1.0 loss: 0.064551495 (learning rate:0.00047708326154743513)\n",
      "4100: accuracy:1.0 loss: 0.0650664 (learning rate:0.0004733312204046323)\n",
      "4100: ********* epoch 1124 ********* test accuracy:0.9972603 test loss: 1.696567\n",
      "4120: accuracy:1.0 loss: 0.02627699 (learning rate:0.0004696165126949802)\n",
      "4140: accuracy:1.0 loss: 0.061537307 (learning rate:0.00046593876694461247)\n",
      "4160: accuracy:1.0 loss: 0.06106198 (learning rate:0.000462297615375889)\n",
      "4180: accuracy:1.0 loss: 0.08755031 (learning rate:0.000458692693870619)\n",
      "4200: accuracy:1.0 loss: 0.017429378 (learning rate:0.00045512364193364755)\n",
      "4200: ********* epoch 1151 ********* test accuracy:0.9972603 test loss: 1.7019544\n",
      "4220: accuracy:1.0 loss: 0.07174826 (learning rate:0.0004515901026568069)\n",
      "4240: accuracy:1.0 loss: 0.102673806 (learning rate:0.00044809172268322453)\n",
      "4260: accuracy:1.0 loss: 0.009738823 (learning rate:0.00044462815217198804)\n",
      "4280: accuracy:1.0 loss: 0.048995517 (learning rate:0.0004411990447631597)\n",
      "4300: accuracy:1.0 loss: 0.047863107 (learning rate:0.00043780405754314123)\n",
      "4300: ********* epoch 1179 ********* test accuracy:0.9972603 test loss: 1.7530752\n",
      "4320: accuracy:1.0 loss: 0.00037966378 (learning rate:0.0004344428510103813)\n",
      "4340: accuracy:1.0 loss: 0.0794908 (learning rate:0.00043111508904142585)\n",
      "4360: accuracy:1.0 loss: 0.06265712 (learning rate:0.0004278204388573046)\n",
      "4380: accuracy:1.0 loss: 0.12004278 (learning rate:0.0004245585709902538)\n",
      "4400: accuracy:1.0 loss: 0.0603545 (learning rate:0.00042132915925076824)\n",
      "4400: ********* epoch 1206 ********* test accuracy:0.9972603 test loss: 1.725642\n",
      "4420: accuracy:1.0 loss: 0.036381703 (learning rate:0.0004181318806949831)\n",
      "4440: accuracy:1.0 loss: 0.030825403 (learning rate:0.0004149664155923781)\n",
      "4460: accuracy:1.0 loss: 0.18930456 (learning rate:0.0004118324473938054)\n",
      "4480: accuracy:1.0 loss: 0.11409788 (learning rate:0.00040872966269983314)\n",
      "4500: accuracy:1.0 loss: 0.034560785 (learning rate:0.00040565775122940656)\n",
      "4500: ********* epoch 1233 ********* test accuracy:0.9972603 test loss: 1.7079277\n",
      "4520: accuracy:1.0 loss: 0.054365657 (learning rate:0.0004026164057888186)\n",
      "4540: accuracy:1.0 loss: 0.09383471 (learning rate:0.0003996053222409906)\n",
      "4560: accuracy:1.0 loss: 0.18336236 (learning rate:0.0003966241994750587)\n",
      "4580: accuracy:1.0 loss: 0.055115677 (learning rate:0.00039367273937626184)\n",
      "4600: accuracy:0.99 loss: 3.1561165 (learning rate:0.0003907506467961309)\n",
      "4600: ********* epoch 1261 ********* test accuracy:0.9972603 test loss: 1.719105\n",
      "4620: accuracy:1.0 loss: 0.21855277 (learning rate:0.0003878576295229724)\n",
      "4640: accuracy:1.0 loss: 0.1704967 (learning rate:0.0003849933982526485)\n",
      "4660: accuracy:1.0 loss: 0.055512093 (learning rate:0.00038215766655964504)\n",
      "4680: accuracy:1.0 loss: 0.09872203 (learning rate:0.0003793501508684298)\n",
      "4700: accuracy:1.0 loss: 0.1840178 (learning rate:0.0003765705704250939)\n",
      "4700: ********* epoch 1288 ********* test accuracy:0.9972603 test loss: 1.75794\n",
      "4720: accuracy:1.0 loss: 0.1977351 (learning rate:0.0003738186472692768)\n",
      "4740: accuracy:1.0 loss: 0.053975526 (learning rate:0.0003710941062063696)\n",
      "4760: accuracy:0.99 loss: 3.0682206 (learning rate:0.00036839667477999555)\n",
      "4780: accuracy:1.0 loss: 0.045295823 (learning rate:0.00036572608324476403)\n",
      "4800: accuracy:1.0 loss: 0.1448099 (learning rate:0.0003630820645392963)\n",
      "4800: ********* epoch 1316 ********* test accuracy:0.9972603 test loss: 1.8039037\n",
      "4820: accuracy:1.0 loss: 0.13426839 (learning rate:0.00036046435425951816)\n",
      "4840: accuracy:1.0 loss: 0.11248435 (learning rate:0.00035787269063222043)\n",
      "4860: accuracy:1.0 loss: 0.23870188 (learning rate:0.0003553068144888804)\n",
      "4880: accuracy:1.0 loss: 0.08581282 (learning rate:0.00035276646923974576)\n",
      "4900: accuracy:1.0 loss: 0.09133926 (learning rate:0.00035025140084817447)\n",
      "4900: ********* epoch 1343 ********* test accuracy:0.9972603 test loss: 1.8310633\n",
      "4920: accuracy:1.0 loss: 0.13819544 (learning rate:0.0003477613578052316)\n",
      "4940: accuracy:1.0 loss: 0.1394609 (learning rate:0.00034529609110453763)\n",
      "4960: accuracy:1.0 loss: 0.098254465 (learning rate:0.0003428553542173683)\n",
      "4980: accuracy:1.0 loss: 0.088581316 (learning rate:0.00034043890306800073)\n",
      "5000: accuracy:1.0 loss: 0.112321086 (learning rate:0.00033804649600930654)\n",
      "5000: ********* epoch 1370 ********* test accuracy:0.9972603 test loss: 1.8486997\n"
     ]
    }
   ],
   "source": [
    "# Adam Optimizer is a backpropagation algorithm that converges faster than standard backpropagation\n",
    "# it will try to minimize cross entropy as loss function with gradient descent given a certain learning rate\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# initilize the TensorFlow graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# these arrays will be filled with evaluation values in function training_step()\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "# start training\n",
    "# print training evaluation every 20 steps\n",
    "# print test evaluation every 100 steps\n",
    "for i in range(generations+1): training_step(i, i % 100 == 0, i % 20 == 0, i == generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VNX5+PHPQxISEvZFUSIEFBfWgAHEhaC1Aoor9itWRK2KIPRX25+t+NWKUhfaan9VsSpfQbRfARGVYhVFUYJL2VR2BCOLREBCgABhCSHP7497J04mM5lJMjfL5Hm/XvNi7j1n7jw3GfLMOefec0RVMcYYY8rToKYDMMYYU/tZsjDGGBOWJQtjjDFhWbIwxhgTliULY4wxYVmyMMYYE5YlC1PriEiciBwSkfa1IJbPROTWmo7DmJpmycJUmfuH3fcoFpEjfts3VfR4qnpCVRur6vdexBsNIvKS3zkWishxv+13qnDccSLyfoR154jIURFpUdn3MyZSlixMlbl/2BuramPge+BKv32vBdYXkfjqjzK6VPUOv3P+C/Ca3zlf6fX7uwliKFAADPf6/QLeu87//kzFWbIwnhORR0XkdRGZKSIHgREi0l9ElojIfhHZKSLPiEiCWz9eRFRE0tzt/3XL54vIQRH5j4h0DPFeDdxv3LvcYy8SkXP8yss9logMFpGNIpIvIk8DUoXzzhSRZW4cX4pIf7+y0SKyzY3hOxG5TkT6AE8Bl7otlJxyDn8jsM2tf0vA+yaIyCMiskVEDrgxtHHLerk/k33uz/0ed/8cERnvd4yhIvKN3/YeEfmdiKwH9rr7HhGRre45rBGRIX71RUR+7f4sD4rIahHp4r7mlYB4XxaRRyv8AzbVS1XtYY+oPYCtwKUB+x4FCoErcb6gNAL6AP2AeKATsAkY59aPBxRIc7f/F9gDZAAJwOvA/4Z4/wbArUATIAmYDKzwKw95LOAk4BBwrVv2e6AIuDXMOT8KTA/YdzqQB1zixnQVsBtoBrTB+YPbya3bDjjbfT4OeD+Cn/NS4CEgDSgGzvIrewRY4f5cGwDnuu/byo1pNNDQ3dfHfc0cYLzfMYYC3/ht7wGWAKcAjdx9w4G2QBxwG3AAaOmW3QZsBnriJNyz3fM83a2X7NZrBOT7zt8etfdhLQtTXT5T1XdUtVhVj6jqclVdqqpFqroZmAJklvP6Oaq6QlWPA68B6cEqucefrqoHVfUo8DBwroikRHCsocBKVX3bLXsKyK3k+d4KzFbVj92Y5gHfApfi/HEXoKuIJKrqD6r6TTnHKkVEzgb6AjNUdSvwH2CkX5U7gPtUdbP73l+qaj5wHbBeVV9Q1UJVzVfV5RU4p7+p6k5VPQKgqrNUdZc6Y0wv4yTDXn4xPKaqq9TxjXue3wGrgKvdelcDGyty/qZmWLIw1WW7/4aInC0i77rdRQeAiUDrcl6/y+/5YaBxsErulVR/EZHN7nGz3SL/Y4c61qn+capqMVBeV1B5OgC3ul1Q+0VkP05SOlVV83C6jn4L/Cgi/xKR0ytw7FuAZarqO7fXgJvdLrg4nG//3wV53Wkh9kcq8Hc4yu1+8p1fGj/9nMt7r1eAEe7zEcA/qxCTqSaWLEx1CZze+EVgLXCGqjbF6VKp9PiAn5HA5TjdP82AM9z9kRx7J84fOecFIg2A1ErGsR14QVWb+z1SVPVZAFWdp6qX4HTN7MDpLoOyP6dS3JhGAN3dRLsL+JMb90BVPeGeR7Dksz3EfnAGypP9ttsGqVMSm4h0Af4G3I7T9dQcpwvS93Mu771mAxe5x7gEmBWinqlFLFmYmtIEp6+6wB2AviuKxz2G0zefDDxWgdf+G0gXkavdK35+izO+UBnTgRtF5GL3G38jEblURE4WkdNE5HIRaQQcxflDfcJ93Y9Aewl9xdHP3JjS/R5dgX/x00D3S8ATIpLmvndvEWkGvAV0cVsEDUWkmYhkuK9ZCVzp7kvFGTspT2Oc7rRcoIGIjMVpWfi8BPy3iPRwB7vPFpF2AKp6AHgXJ0l8pKqV7eoz1ciShakp/xfnj9tBnFbG61E67ss439R3AOuALyJ9oar+CNwA/BUn2bTHGUiuMFX9FvgFzuB3Hs637l/jfPOOBx7ASQx7cP7g/8Z96XvAD0CuiGwLcuhbgNdVdZM7XrBLVXcBzwDDRKSx+54LgCxgP/APoKHb/fVznJZJLrABON897v/gXF21HZgHzAhzfsuAqcDXOD/rdjgJx2c68CzwJs6A9utAU7/yV4DuWBdUnSGqtviRMaZ6uV1QXwBt3QsRTC1nLQtjTLVyB+F/C7xqiaLusDsxjTHVRkROArbgXEY8qIbDMRVg3VDGGGPCsm4oY4wxYcVMN1Tr1q01LS2tpsMwxpg65csvv9yjqmEvEY+ZZJGWlsaKFStqOgxjjKlTQlyiXYZ1QxljjAnLkoUxxpiwLFkYY4wJy5KFMcaYsCxZGGOMCcuzZCEi00Rkt4isDVEu4ixvme0uudjbr+wWEfnWfdwS7PXGGGOqj5cti+nA4HLKhwCd3cco4HkAEWkJTMBZcrMvMEGcxemNMcbUEM/us1DVxSKSVk6Vq3EmElNgiYg0F5FTgIHAh6rqWxT+Q5ykM9OrWL32+eef88EHHwQt69evH1dccUVEx8nJyWHq1KmcOHEifGVjTLVKTExk7NixNG/ePKL6r732Ghs3bozKe6empjJq1KioHCuUmrwprx2ll2nMcfeF2l+GiIzCaZXQvn17b6KMgjFjxrBmzRpESi/Wpqo0b96cPXv2EBcXF/Y4Tz31FH//+9/LHMcYU/NUlcaNG/Ob3/wmbN28vDxGjHBWlo3G/+d+/frFdLII9hPScvaX3ak6BZgCkJGRUStnRNyzZw9r1qzh0Ucf5YEHHihV9s9//pORI0eyZs0a0tPTwx5r0aJFXHzxxXz88cdehWuMqaSOHTuSlZUVUbJYvHgxAJ9++ikXXnih16FFRU1eDZWD33rHOGsd7yhnf5306aefApCZmVmmzLcvKysr7HH27dvHqlWrgh7HGFPzMjMzWbx4McXFxWHrZmVlkZSURJ8+faohsuioyWQxDxjpXhV1HpCvqjuBD4DLRKSFO7B9mbuvTirvQ9G+fXvS0tIiShafffYZqmrJwphaKjMzk7y8PNavXx+2blZWFv379ycxMbEaIosOLy+dnQn8BzhLRHJE5HYRGS0io90q7wGbgWyc9X/vBnAHtv8ELHcfE32D3XVRuA9FpN9GsrKyaNiwIf369fMiTGNMFUXaU1BXewk8SxaqeqOqnqKqCaqaqqpTVfUFVX3BLVdVHauqp6tqd1Vd4ffaaap6hvt42asYvRbJhyLSbyNZWVn069ePRo0aRTtMY0wUdOzYkdTU1LDJoq72EsTMFOW1USQfioEDBwJOMujWrZuz86OPYNIk6NIFnnmGAwcOMGbFCjLT0uBnP/vpxX36OPUAbrwRdu8uffABA2DCBOf5NdfAwYOlywcNgj/8wXl+2WUQeEnutdfCuHFQWAhDhpQN/pe/hNtvh/x8uO66suV33OHEtXMnuFd+lPLrXztxbd4Md95ZtvwPf3BiXLsWgg0aPvwwXHQRLFsG999ftvwvf4Fzz4WsLJg4sWz5s886P+P334e//rVs+UsvQceO8NZb8NxzZctnzICTT4bXXoNp08qWv/02NG0K//M/MGtW2fIPPoD4eHjmGfjXv0qXJSQ4cQH8+c+wYEHp8qZNneOD8zv+7LPS5W3bOnEB/P738NVXpcs7dnTOD5zf8YYNpcu7dnXiAud3vHVr6XL77JX57Anw/vHj7J07F12xAsnICPrZOzs7m54JCZx33nnR++zNnw8NG5atF0WWLDyUlZVFYmKi86EIIS0tjdNOO42srCzGjh3r7PznP+GLL6BzZ8C5TyMeaNm4sfOfx+f48dLP/csCywsLy5YXFZUuD/wP6ytXLfta+Kl+VcuLi4OX+7rmQr2+quW+JYVDvX+k5SdOBC/3qWp5UVH5v9vKlPv/7sN9dqpaXo8+e62aNGHvjz+ybcsW0jIygr7+0N699OzRg6SkpOh99qpjeWxVjYnHueeeq7VNRkaGDhgwIGy9ESNG6EknnaTFxcXOjosuch6u++67T+Pj4/XQoUNehWqMiYKNGzcqoC+88ELQ8vz8fG3QoIH+8Y9/rObIQgNWaAR/Y20iQY8cOHCAr776KqJ+yczMTHbv3v3T3ZxbtjhNUFdWVhZ9+vQhJSXFq3CNMVHQuXNn2rZtG3Lc4vPPP6e4uLjOjVeAzTrrmYp8KEpdRVFUBMeOlSSLgoICVqxYUSc/XMbUNyJCZmYmWVlZaJCuoaysLBISEujfv38NRFc1liw8UpEPxRlnnMEpp5ziJIv4eGew8I9/BOCLL76gqKioZCDcGFO7DRw4kB07dvDdd9+VKfP1EiQnJ9dAZFVjycIjFflQBP024s4VlZWVRVxcHOeff76X4RpjoiTU/RZ1vZfAkoUHKvOhyMzMZMeOHex68UUYPhwOHACcD9y5555LkyZNvArXGBNFZ599NieddFKZZOHrJbBkYUpU5kPhq/vjvHnOtdUpKRw+fJilS5fW2Q+XMfWRiDBgwAAWLVpUatxi0aJFdbqXwJKFByrTdeT7NnJ0/Xpo3x7i4liyZAnHjx+3ZGFMHZOZmcn27dvZ6nczY13vJbBk4YHKfCh830YSd+5E3SuhsrKyaNCgQZ2ZwtgY4wgctzh8+DDLli2r01/8LFlE2ZEjRyr9oRg4cCDtCgs52Lo14HzQ0tPTadasWbTDNMZ4qGvXrrRs2bIkWfh6CeryVY2WLKJsyZIlFBYWVi5ZnH8+O4ANIhw9epQlS5bU6W8ixtRXDRo0YMCAASXJIhZ6CSxZRFlVPhTn9OzJpa1a8UJiIsuWLePYsWOWLIypozIzM9myZQvbt28nKyuLXr160bRp05oOq9IsWUTZokWL6NWrV6W6jvy/jWRlZSEiXHTRRR5EaYzxmu+L3oIFC2Kil8CSRRRVuevolVeYvHIlu7dsYcaMGXTv3p2WLVtGN0hjTLXo0aMHzZo146mnnoqJXgJPk4WIDBaRjSKSLSLjg5R3EJGFIrJaRBaJSKpf2Z9FZK37uMHLOKOlyl1Hq1Zx8s6dFADffPNNnf9wGVOfxcXFcdFFF7Fhw4aY6CXwclnVOOA5YAjQBbhRRLoEVHsSeFVVewATgSfc114B9AbSgX7A70Wk1nf2VbnraMsWGpx+Os2bNwewZGFMHef7P9yjRw9atGhRw9FUjZcti75AtqpuVtVCYBZwdUCdLsBC9/knfuVdgCxVLVLVAmAVMNjDWKNi8eLFVftQbNmCdOxYkmwGDBgQxeiMMdXNlyxi4Yufl8miHbDdbzvH3edvFTDMfX4t0EREWrn7h4hIsoi0Bi4GTgt8AxEZJSIrRGRFbm5u1E+gorZv385ZZ51VuRerlqxj8fvf/55HH32UNm3aRDdAY0y16t27N7/73e8YPXp0TYdSZV4uqypB9gVO8H4vMFlEbgUWAz8ARaq6QET6AF8AucB/gKKA16KqU4ApABkZGdWwrmD5jhw5QqNGjSr34sJCZ03fPn246KKL6nz/pjHGGbd46qmnajqMqPAyWeRQujWQCuzwr6CqO4DrAESkMTBMVfPdsseAx9yyGcC3HsYaFVVKFomJ8O9/RzcgY4yJEi+7oZYDnUWko4g0BIYD8/wriEhrEfHFcD8wzd0f53ZHISI9gB7AAg9jjYoqJQtjjKnFPEsWqloEjAM+ADYAs1V1nYhMFJGr3GoDgY0isgk4GbclASQAn4rIepxuphHu8Wq1KiWLv/3NmW328OHoBmWMMVHgZTcUqvoe8F7Avof8ns8B5gR53VGcK6LqjOPHj3PixInKJ4vsbDh0COrgcovGmNhnd3BHyZEjRwAqnyzcK6GMMaY2smQRJZYsjDGxzJJFlFQpWRQXw9atkJYW1ZiMMSZaLFlESZWSxbFjcNttUIcXRjHGxDZPB7jrkyoli0aN4PnnoxyRMcZEj7UsoqRKyeLIEThxIsoRGWNM9FiyiJIqJYunnnJaF0ePRjkqY4yJDksWUVKlZLFlC7RqBUlJUY7KGGOiw5JFlFQ5WdiVUMaYWsySRZRUOVnYPRbGmFrMkkWUVDpZFBXB9u2WLIwxtZpdOhsllU4Wx4/DxInOWhbGGFNLWbKIkkoni0aN4L//24OIjDEmeqwbKkqOHDmCiNCwYcOKvXD3bsjJcZZVNcaYWsqSRZT41rIQCbaabDmefda5EspuyjPG1GKWLKKk3IWPioqcNbaD2bIFUlMh3noEjTG1l6fJQkQGi8hGEckWkfFByjuIyEIRWS0ii0Qk1a/sLyKyTkQ2iMgzUuGv7NWr3GQxciT07g15eWXLtm61K6GMMbWeZ8lCROKA54AhOKve3SgigavfPQm8qqo9gInAE+5rzwcuwFl7uxvQB8j0KtZQtALjCOUmi61bYd06uPLKssum2j0Wxpg6wMuWRV8gW1U3q2ohMAu4OqBOF2Ch+/wTv3IFkoCGQCLOmtw/ehhrGcXFxXTs2JGpU6dGVL/cZHHoELRr5yydunnzT/uPHoUdOyxZGGNqPS87ytsB2/22c4B+AXVWAcOAp4FrgSYi0kpV/yMinwA7AQEmq+qGwDcQkVHAKID27dtHNfj9+/ezbds2Nm7cGFH9cpPFY49B48aQkQFNmpQu++c/oUePKkZrjDHe8jJZBBtjCOzXuReYLCK3AouBH4AiETkDOAfwjWF8KCIDVHVxqYOpTgGmAGRkZET12tPc3FwADgd2G4VQbrK48sqfnqs6N+EVF8Mjj8CIEVUN1RhjPOdlssgBTvPbTgV2+FdQ1R3AdQAi0hgYpqr5bothiaoecsvmA+fhJJRq4UsWBQUFEdU/cuQIbdq0CV64eLHT1XSa++P4/nuYNg0KCuD666FvX2hgF6YZY2ovL/9CLQc6i0hHEWkIDAfm+VcQkdYi4ovhfmCa+/x7IFNE4kUkAWdwu0w3lJei1rIoLobMTHjpJWdbBF58Ea64wlnHon//aIVsjDGe8SxZqGoRMA74AOcP/WxVXSciE0XkKrfaQGCjiGwCTgYec/fPAb4D1uCMa6xS1Xe8ijWYyrQsgiYLdxoQGjf+aV98PMyeDaecAj17WqvCGFPreXonmKq+B7wXsO8hv+dzcBJD4OtOAHd5GVs4UWtZHDrk/JuSUnp/cjJs2xb6Zj1jjKlF7LbhEKLWsvC93r9l4ZOQ4DyMMaaWs/6PEKLesgiWLIwxpo6wlkUIFWlZFBcXc+zYseDJokMHmDsX+vSJdojGGFNtLFmEUJGWxdGjR4EQa1k0awZXB964bowxdYt1Q4VQkZZFuQsf/fADvPvuT91RxhhTB1myCEJVSyWLcBMKlpssPvkEhg6FnTujHqcxxlQXSxZBHDx4kMLCQlq2bImqcuzYsXLrl5ssbIDbGBMDLFkE4WtVpKWlAeG7ospNFr7XBt5nYYwxdYgliyB8yaJDhw5A+EHuiFoWliyMMXWYJYsgot6ySEqCuLioxmiMMdXJkkUQgckiXMvCVx40WYweDfPnRzU+Y4ypbnafRRBRbVl06uQ8jDGmDrOWRRC5ubk0atSoZH2KKo1ZfPIJLFgQ9RiNMaY6WcsiiNzcXNq0aUNycjJQxZbFn/8M+/fDZZdFPU5jjKku1rIIwpcsUtwrmKrUsigosCuhjDF1nqfJQkQGi8hGEckWkfFByjuIyEIRWS0ii0Qk1d1/sYis9HscFZFrvIzVX2CyqFLL4tAhuyHPGFPneZYsRCQOeA4YAnQBbhSRLgHVngReVdUewETgCQBV/URV01U1HbgEOAxUW8d/YDdUle+zsJaFMaaO87Jl0RfIVtXNqloIzAICp1/tAix0n38SpBzgemC+qka2sEQUVGbMomHDhjQItjxqQYG1LIwxdZ6XA9ztgO1+2zlAv4A6q4BhwNPAtUATEWmlqnl+dYYDfwv2BiIyChgF0L59+6gEffjwYQ4fPkybNm1ISEggISEhopZF0FYFwIcfWsvCGFPnedmykCD7AqdvvRfIFJGvgUzgB6Co5AAipwDdgQ+CvYGqTlHVDFXN8F3mWlW+eyx8x0tJSYmoZREyWXTtCu79GsYYU1d5mSxygNP8tlOBHf4VVHWHql6nqr2AB9x9+X5V/gt4W1WPexhnKYHJIjk5ufIti8JCmDwZVq+OepzGGFOdvEwWy4HOItJRRBridCfN868gIq1FxBfD/cC0gGPcCMz0MMYyotqyyM+HX/8asrKiHqcxxlQnz5KFqhYB43C6kDYAs1V1nYhMFJGr3GoDgY0isgk4GXjM93oRScNpmVTrX9qotix8ScYGuI0xdVzYAW4RGQe8pqr7KnpwVX0PeC9g30N+z+cAc0K8divOIHm12r17NxClloVNT26MiRGRtCzaAstFZLZ7k12wgeuYkZubS0JCAk2bNgWsZWGMMRBBslDVB4HOwFTgVuBbEXlcRE73OLYa4bvHwpcTo9KysGRhjKnjIhqzUFUFdrmPIqAFMEdE/uJhbDXClyx8qtSyuOACyM6GjIxoh2mMMdUqkjGL/wPcAuwBXgJ+r6rH3auYvgX+4G2I1SswWVSpZZGUBKfHZAPMGFPPRNKyaA1cp6qDVPUN3z0PqloMDPU0uhoQ1ZbFl186U5SHSTbGGFPbRZIs3gP2+jZEpImI9ANQ1Q1eBVZTotqy+PRTGD8ejlfbPYXGGOOJSJLF88Ahv+0Cd1/MOXbsGAcPHizTsjh+/DjHQ/zBV1W7dNYYE/MiSRbiDnADJd1PMbnCXuANeUDYBZAKCwtR1dCXziYkOA9jjKnDIkkWm0Xk/4hIgvv4DbDZ68BqQrBkEW5NC1v4yBhTH0SSLEYD5+PMCOubZnyUl0HVlPJaFqHGLWxJVWNMfRC2O0lVd+NMAhjzot6ymDzZroQyxsSESO6zSAJuB7oCSb79qvorD+OqEVFvWSQnOw9jjKnjIumG+ifO/FCDcGaATQUOehlUTcnNzSUuLo4WLVqU7KtSy+If/4Dp06MepzHGVLdIksUZqvpHoEBVXwGuwFm9Lubk5ubSqlWrUmtpV6llMXUqzAk6qa4xxtQpkSQL3w0G+0WkG9AMSPMsohoUeEMeVLFlYQPcxpgYEcn9ElNEpAXwIM5Kd42BP3oaVQ0Jliyq1LKwS2eNMTGi3JaFO1ngAVXdp6qLVbWTqp6kqi9GcnB3/YuNIpItIuODlHcQkYUislpEFolIql9ZexFZICIbRGS9u3Kep6xlYYwxwZWbLNy7tcdV5sAiEgc8BwwBugA3ikiXgGpPAq+qag9gIvCEX9mrwF9V9RygL7C7MnFURNRbFgUF1rIwxsSESLqhPhSRe4HXceaFAkBV94Z+CeD8gc9W1c0AIjILuBpY71enC/Bb9/knwFy3bhcgXlU/dN/Lf24qTxw/fpx9+/aVSRaJiYmISOWSxZEjcOJE1GM1xpjqFskA96+AscBi4Ev3sSKC17UDtvtt51B2Te1VwDD3+bVAExFpBZyJM6D+loh8LSJ/dVsqpYjIKBFZISIrfPdIVFZeXh5AmWQhIqSkpFSuGyouDho2rFJcxhhTG0SyrGrHII9OERw72FrdGrB9L5ApIl8DmThTihThtHgucsv7AJ1wlnQNjG2KqmaoakbgH/mKCnZDnk9ycnLFWxZ5eTBmDCxdWqW4jDGmNojkDu6Rwfar6qthXpoDnOa3nQrsCDjGDuA6930aA8NUNV9EcoCv/bqw5gLn4awD7onykkW4lkWDBg1ICJxZdvdueOEFGDAA+vWLerzGGFOdIhmz6OP3PAn4GfAVzgB0eZYDnUWkI06LYTjwS/8KItIa2OsOpN8PTPN7bQsRaaOqucAlRNb1VWlVaVk0atQIkYCGlK++DXAbY2JAJBMJ/tp/W0Sa4UwBEu51RSIyDvgAiAOmqeo6EZkIrFDVecBA4AkRUZwxkbHua0+4g+oLxfkr/CXwPxU6swqqSsvCFj4yxsS6yixidBjoHElFVX0PZ1lW/30P+T2fAwSdD8O9EqpHJeKrFF+yaNWqVZmySFoWZVjLwhgTQyIZs3iHnwamG+Bc7jrby6BqQm5uLi1btiQ+vuyPJCUlhV27dgV9XchkUVgIiYnWsjDGxIRIWhZP+j0vArapao5H8dSY3NxcTjrppKBllWpZXHstHD0azRCNMabGRJIsvgd2qupRABFpJCJpqrrV08iqWbC7t30qNWZhjDExJJKb8t4Aiv22T7j7Ykp5yaJSLYu334aRI6GoKJphGmNMjYgkWcSraqFvw30ec7clR71l8eWXMGOGcxe3McbUcZEki1wRucq3ISJXA3u8C6n6FRcXk5eXV27L4siRIxQXF5cpK/fS2caNIfD+C2OMqYMiGbMYDbwmIpPd7Rwg6F3dddXevXspLi4ut2UBTmJICbi66ciRIyXTmJdi05MbY2JIJDflfQec507HIaoac+tvl3dDHvy0pkVBQUHQZGELHxljYl3YbigReVxEmqvqIVU9KCItROTR6giuuoRLFr4EEWzcImSyaNQI2gVOsmuMMXVTJGMWQ1R1v29DVfcBl3sXUvWrSMsiUMhkMW0afPxx9II0xpgaFEmyiBORRN+GiDQCEsupX+dUtmVx4sQJjh8/bvdZGGNiXiQD3P+LM6Hfy+72bcAr3oVU/XzJonXr1kHLQ7Usyl346K67oHNnuPfeKEZqjDE1I5IB7r+IyGrgUpwFjd4HOngdWHXKzc2lWbNmNAyxql2olkW5yeL99+HYsegGaowxNSSSbiiAXTh3cQ/DWc9ig2cR1YDybsiDSrYs7NJZY0wMCdmyEJEzcRYsuhHIA17HuXT24mqKrdqESxaValnYpbPGmBhSXsviG5xWxJWqeqGqPoszL1TMiXrLoqjI6YKyloUxJkaUlyyG4XQ/fSIi/yMiP8MZs4iYiAwWkY0iki0i44OUdxCRhSKyWkQWiUiqX9kJEVnpPuZV5H0rKuoti2PH4Jxz4NRToxuoMcbUkJDdUKr6NvC2iKQA1wC/BU4WkeeBt1V1QXkHFpE44Dng5zhThCwXkXmqut6v2pPAq6r6iojbwsEkAAAeb0lEQVRcAjwB3OyWHVHV9MqeWKRUlT179pSbLHzJIOKWRUoKrF+PMcbEirAD3KpaoKqvqepQIBVYCZRpJQTRF8hW1c3uTLWzgKsD6nQBFrrPPwlS7rn8/HyOHz9ebrKIi4sjKSmpYmMWxhgTQyK9GgoAVd2rqi+q6iURVG8HbPfbznH3+VuF090FcC3QRER8i2AnicgKEVkiItcEewMRGeXWWeG7V6KiRITx48fTv3//cusFW9MiZLL45hu46CL4z38qFZMxxtQ2kdyUV1nBxjc0YPteYLKI3AosBn7AWboVoL2q7hCRTsDHIrLGndTwp4OpTgGmAGRkZAQeOyLNmjXjiSeeCFsv2JoWIZNFbi589plz+awxxsQAL5NFDnCa33YqsMO/gqruAK4DcGe1Haaq+X5lqOpmEVkE9AJKJYvqVKGWha+eXQ1ljIkRFeqGqqDlQGcR6SgiDXHu2Sh1VZOItBYRXwz3A9Pc/S1881GJSGvgAqBGR4wr1LI4dMj51+6zMMbECM+ShaoWAeOAD3Du+J6tqutEZKLfynsDgY0isgk4GXjM3X8OsEJEVuEMfE8KuIqq2lnLwhhTn3nZDYWqvge8F7DvIb/nc4A5QV73BdDdy9gqKiUlhf3795fa50sWSUlJpSs3bQrnnuv8a4wxMcDLbqiYkpKSErRlkZSUhASus33ttbBiBYSYxdYYY+oaSxYRSk5ODjpmYfdYGGPqA0sWEQrVsgiaLP78Z+c+C2OMiRGWLCJUoZbF5s3w7bfVFJkxxnjPkkWEfC0L1Z/u/QuZLAoK7LJZY0xMsWQRoeTkZIqLiznmt/pdyGRx6JBdNmuMiSmWLCIUbJpya1kYY+oLT++ziCX+CyC1bNkScJJFixYtylbu0QMCL6c1xpg6zJJFhEK1LE4NtsDRU09VV1jGGFMtrBsqQsGWVrX7LIwx9YUliwhVaMwiPR0mTKiu0IwxxnOWLCJUoZbFt9/aWhbGmJhiySJCEbcsiovh8GG7dNYYE1MsWUQosGWhqsGThS+Z2KWzxpgYYskiQoEtC9/NeSEXPrKWhTEmhniaLERksIhsFJFsERkfpLyDiCwUkdUiskhEUgPKm4rIDyIy2cs4IxHYsgi58FFcHFx/PZx1VrXGZ4wxXvLsPgsRiQOeA36Osx73chGZF7Di3ZPAq6r6iohcAjwB3OxX/icgy6sYKyKwZREyWbRpA2+8Ua2xGWOM17xsWfQFslV1s6oWArOAqwPqdAEWus8/8S8XkXNxllpd4GGMEUtISCA+Pj58y8IYY2KQl8miHbDdbzvH3edvFTDMfX4t0EREWolIA+Ap4PflvYGIjBKRFSKyIjc3N0phh5aSkhK+ZbFwIbRoAcuXex6PMcZUFy+TRbDJkTRg+14gU0S+BjKBH4Ai4G7gPVXdTjlUdYqqZqhqRps2baIRc7mSk5PDtywOHID9+yEhwfN4jDGmung5N1QOcJrfdiqww7+Cqu4ArgMQkcbAMFXNF5H+wEUicjfQGGgoIodUtcwgeXWKqGXhuxnPLp01xsQQL5PFcqCziHTEaTEMB37pX0FEWgN7VbUYuB+YBqCqN/nVuRXIqOlEARG2LOzSWWNMDPKsG0pVi4BxwAfABmC2qq4TkYkicpVbbSCwUUQ24QxmP+ZVPNFgLQtjTH3l6RTlqvoe8F7Avof8ns8B5oQ5xnRgugfhVVhELYtzzoFbbgH3vgxjjIkFtp5FBaSkpJCXlweUkywuv9x5GGNMDLHpPiogopZFcXF1h2WMMZ6zZFEBEY1Z3HornHlmNUdmjDHesmRRARG1LAoKoGHD6g7NGGM8ZcmiAgJbFvHx8cTHBwz7HDpkV0IZY2KOJYsKSE5OprCwkKKiotCr5BUU2D0WxpiYY8miAvxnng2ZLKxlYYyJQXbpbAX4r2kRMlncdBO0bl3NkRljjLcsWVRARC2L35c7Ua4xxtRJ1g1VARG1LPLzoaiomiMzxhhvWbKogLAtC1Vo2RIeeaQGojPGGO9YsqiAsC2Lo0edO7jtaihjTIyxZFEBYVsWNuOsMSZGWbKogLAtC1vLwhgToyxZVEDYloUlC2NMjLJkUQFhWxatWsHDD0P37tUfnDHGeMjT+yxEZDDwNBAHvKSqkwLKO+AspdoG2AuMUNUcd/9b7usSgGdV9QUvY41E2JbFKafAhAk1EJkx5Tt+/Dg5OTkcPXq0pkMxNSQpKYnU1FQSEhIq9XrPkoWIxAHPAT8HcoDlIjJPVdf7VXsSeFVVXxGRS4AngJuBncD5qnpMRBoDa93X7vAq3kgkJSUhIqFbFgUFsH8/nHwyBE4waEwNysnJoUmTJqSlpSEiNR2OqWaqSl5eHjk5OXTs2LFSx/CyG6ovkK2qm1W1EJgFXB1Qpwuw0H3+ia9cVQtV9Zi7P9HjOCMmIiQnJ5Ofn09RUVHZZPGvf0FqKnz3Xc0EaEwIR48epVWrVpYo6ikRoVWrVlVqWXr5R7gdsN1vO8fd528VMMx9fi3QRERaAYjIaSKy2j3Gn4O1KkRklIisEJEVubm5UT+BYJKTk0uWVg156awNcJtayBJF/VbV37+XySJYZBqwfS+QKSJfA5nAD0ARgKpuV9UewBnALSJycpmDqU5R1QxVzWjTpk10ow/Bfx3ukFdD2X0WxpgY42WyyAFO89tOBUq1DlR1h6pep6q9gAfcffmBdYB1wEUexhqx5ORk9uzZA1jLwphI5eXlkZ6eTnp6Om3btqVdu3Yl24WFhREd47bbbmPjxo0Rv+fOnTu5/PLL6dmzJ126dOGqq64qt/7evXt54YXyr6N54403EBGys7MjjiNWeJkslgOdRaSjiDQEhgPz/CuISGsR8cVwP86VUYhIqog0cp+3AC4AIv+UeChsy6JhQ6jk1QbGxKpWrVqxcuVKVq5cyejRo/ntb39bst3QXYZYVSkuLg55jJdffpmzzjor4vd88MEHueKKK1i1ahXr16/n0UcfLbd+JMli5syZXHjhhcyaNSviOCqjqBZORurZJTuqWiQi44APcC6Bnaaq60RkIrBCVecBA4EnRESBxcBY9+XnAE+5+wV4UlXXeBVrRZTbshg6FNoFDssYU7vcc889rFy5MqrHTE9P5+9//3uFX5ednc0111zDhRdeyNKlS/n3v//NI488wldffcWRI0e44YYbeOihhwC48MILmTx5Mt26daN169aMHj2a+fPnk5yczL/+9S9OOumkUsfeuXMnqampJds9evQoeT5p0iTeeustjh49yvXXX89DDz3E+PHj2bhxI+np6QwePJhJk0pd6c+BAwdYunQpCxcuZNiwYTz44IMlZY8//jgzZ86kQYMGDB06lMcee4xNmzYxevRo8vLyiIuL46233iI7O5vJkyczd+5cAEaPHs2FF17IiBEjSE1N5a677uL999/nnnvuIS8vj6lTp1JYWMiZZ57Jq6++SqNGjdi1axd33XUXW7ZsQUSYMmUKc+fOJTU1lbFjnT+h9913Hx06dODuu++u8O8kFE+v71TV94D3AvY95Pd8DjAnyOs+BHoE7q8NUlJSOHjwIBAkWVx4ofMwxkRs/fr1vPzyyyXf6idNmkTLli0pKiri4osv5vrrr6dLly6lXpOfn09mZiaTJk3id7/7HdOmTWP8+PGl6owbN45f/vKX9O7dm0svvZTbbruNU045hffee4/vv/+epUuXoqpcfvnlfPHFF0yaNIns7OyQifStt95i6NChnH322aSkpLB69Wp69OjBO++8w/z581m2bBmNGjVi7969ANx44408/PDDXHnllRw9epTi4uKw3VcpKSl8/vnngNN1N3r0aADGjx/P9OnTGTNmDGPHjuXnP/8548aNo6ioiMOHD9O6dWuGDx/O2LFjOXHiBG+88QZffvllxX8Z5bCbASrIdxc3BEkWW7c6/6alVVs8xlRUZVoAXjr99NPp06dPyfbMmTOZOnUqRUVF7Nixg/Xr15dJFo0aNWLIkCEAnHvuuXz66adljnv55Zfz3Xff8f777zN//nx69erFunXrWLBgQck2wKFDh9i0aVOZlkmgmTNnliSk4cOHM3PmTHr06MFHH33Er371q5K/By1btmTfvn3s2bOHK6+8EnDu0YrEDTfcUPJ89erVPPTQQ+zfv5+DBw8ydOhQABYtWlTSDRYfH0/Tpk1p2rQpTZo0Yc2aNWzbto2+ffvSokWLiN4zUpYsKijFb/C6TLK4+27IzYXly6s5KmPqLv//U99++y1PP/00y5Yto3nz5owYMSLovQG+cQ6AuLi4kH38rVq14qabbuKmm25i8ODBfPbZZ6gqDz74ILfffnupuuV968/NzSUrK4tvvvkGEaGoqIiEhAQef/xxVDXoZanB9sXHx5calwk8N/+fxciRI5k/fz7dunXjpZdeYsmSJeUe+/bbb2f69Ols3bqVu+66K+S5VFatuNmtLim3ZVFQYFdCGVMFBw4coEmTJjRt2pSdO3fywQcfVPpYCxcu5MiRIyXH3bJlC+3bt2fQoEFMnTqVAvfqxZycHPbs2UOTJk1KupgDzZ49m9tvv51t27axdetWcnJyOPXUU1myZAmXXXYZU6dOLXmvvXv30qJFC1q3bs0777wDOEnh8OHDdOjQgXXr1lFYWMi+ffv4+OOPQ8ZfUFBA27ZtOX78ODNmzCjZf/HFF5d02Z04cYIDBw4AMGzYMN555x1WrlzJpZdeWumfWyjWsgBYvBiCXSnx9NNwzjnw/vvwt78BcM+mTVzjFjdxr4ri7bfh+edh5Uq4qFZc4WtMndS7d2+6dOlCt27d6NSpExdccEGlj7V8+XLGjRtHQkICxcXFjBkzhl69etGrVy+++eYbzjvvPACaNGnCjBkzSEtLIyMjg+7du3PFFVeUGuCeOXMmDz/8cKnjDxs2jBkzZvDss8+yatUqMjIySEhI4Morr+RPf/oTr732GnfddRcPPPAADRs25M0336Rjx45cc801dO/enTPPPJPevXuHjH/ixIn07duX9u3b061bt5JWyOTJk7nzzjt58cUXiY+P58UXX6Rv374kJSUxYMAA2rZtS4MG0W8HiGrgfXJ1U0ZGhq5YsaJyL/74Y/C7sqHElCnQrRv8+9/w+OMAfL99Ozk5OQCc/sUXnNy/P8yeDb5+4NGjYeTIysVhjEc2bNjAOeecU9NhGA8VFxeTnp7O3Llz6dSpU9A6wT4HIvKlqmaEO761LAAuuQS++CJ0+dChzgOY9Ze/cN999wGw9+yznfL/+i/nYYwxNWDNmjVcddVV/OIXvwiZKKrKkkUFlTtmYYwxNaB79+5s2bLF0/ewAe4K8l2tICIkJibWcDTGGFM9LFlUkK9l4Vvbwhhj6gNLFhXka1lYF5Qxpj6xZFFBvpaFJQtjTH1iyaKCrGVhTMVFY4pygGnTprFr166gZZ9//jn9+vUjPT2dc845hz/96U/lHuurr77i/fffL7fO2LFjad++PbFyi0FV2NVQFWTJwpiK801RDvDwww/TuHFj7r333gofZ9q0afTu3Zu2bduWKbvllluYO3cu3bp148SJE2HXvvjqq69Yu3YtgwcPDlp+4sQJ5s2bx6mnnsrnn3/OhR5NEqqqqKonN9JFU+2OrhaybigTEwYOLPv4xz+cssOHg5dPn+6U79lTtqwKXnnlFfr27Ut6ejp33303xcXFFBUVcfPNN9O9e3e6devGM888w+uvv87KlSu54YYbgrZIcnNzS5JIXFxcyeSDhw4d4tZbb6Vv37706tWLd955hyNHjjBx4kRee+010tPTmTOnzOTXfPTRR/Tq1YtRo0Yxc+bMkv0HDx7klltuoXv37vTo0aNkuvF3332X3r1707NnTy677DLAWVPDf+LGs88+m5ycHLKzs+nWrRujR4+md+/e7Ny5k1GjRpGRkUHXrl2ZOHFiyWuWLl1K//796dmzJ/369ePw4cOcf/75rF27tqROv379WLduXZV+D+FYy6KCrGVhTPSsXbuWt99+my+++IL4+HhGjRrFrFmzOP3009mzZw9r1jjL2Ozfv5/mzZvz7LPPMnnyZNLT08sc65577qFz585cfPHFDBkyhJEjR5KYmMjEiRMZPHgw06dPZ9++ffTr169kRte1a9eGnIV35syZ3HjjjQwZMoQJEybw9NNPEx8fz8MPP0ybNm1Ys2YNqsr+/fvZtWsXY8aM4dNPP6VDhw4l05SXJ5Kp2Tt16sTw4cN588036d27N/n5+SQmJpZMGvjkk0+yfv16ALp27VrZX0NELFlUkLUsTExYtCh0WXJy+eWtW5dfXgEfffQRy5cvJyPDmW3iyJEjnHbaaQwaNIiNGzfym9/8hssvv7zkm3p5HnnkEW6++WYWLFjAq6++yuuvv85HH31UMiW5b66no0eP8v3335d7rGPHjrFgwQKee+45UlJS6N27NwsXLmTQoEF89NFHJa0JEaFFixa8/fbbXHzxxXTo0AFwpikPJ5Kp2Y8dO0b79u1L5pBq1qwZ4EyRnp6ezqRJk5g2bRq33XZb2PerKk+ThYgMBp7GWSnvJVWdFFDeAWcp1TbAXmCEquaISDrwPNAUOAE8pqqvexlrpCxZGBM9qsqvfvWroIPRq1evZv78+TzzzDO8+eabTJkyJezxzjjjDM444wzuvPNOWrVqRX5+PqrK3LlzOf3000vVXbx4ccjjvPvuu+Tn55d8Wy8oKKBly5YMGjQo6JTkoaYpL29K8kimZg913JSUFAYOHMi8efN48803o77yYTCejVmISBzwHDAE6ALcKCJdAqo9Cbyqqj2AicAT7v7DwEhV7QoMBv4uIs29irUi4uLiSExMtGRhTBRceumlzJ49u2Sp4ry8PL7//ntyc3NRVX7xi1+ULLMKlDuN+Lvvvlty1dKmTZtITEykSZMmDBo0iGeeeaak3tdffx32WDNnzixZG2Lr1q1s3ryZ+fPnc/ToUS677DImT54MOEli3759XHDBBXz88cds27YNoKQbKi0trWTFumXLlrF9+/ag7xdqavauXbuybdu2kvM/cOAAJ06cAOCOO+5g3LhxnH/++SUtDi95OcDdF8hW1c2qWgjMAq4OqNMFWOg+/8RXrqqbVPVb9/kOYDdO66NWSElJsWRhTBR0796dCRMmcOmll9KjRw8uu+wyfvzxR7Zv386AAQNIT0/nzjvv5HF31ufbbruNO+64I+gA9/Tp0znrrLNIT0/n1ltvZcaMGTRo0IAJEyZw+PBhunfvTteuXUumGr/kkktYtWoVvXr1KjXAfejQIRYuXFiyEh84iaVfv368++67TJgwgR9//JFu3bqRnp7Op59+ysknn8zzzz/P1VdfTc+ePbnpppsA+MUvfsGPP/5Ir169mDp1ashJ/vynZr/zzjtLpmZPTExk5syZjBkzpmTg/NixY4AzqJ2cnFwtXVDg4RTlInI9MFhV73C3bwb6qeo4vzozgKWq+rSIXAe8CbRW1Ty/On2BV4Cuqloc8B6jgFEA7du3P9eX1b324osv0rNnz5L58I2p7WyK8tizfft2fv7zn7Nhw4aIpx6qyhTlXrYsgkUfmJnuBTJF5GsgE/gBKFkfUUROAf4J3BaYKABUdYqqZqhqRps21dfwuOuuuyxRGGNqzMsvv8z555/P448/Xm1z1Hk5wJ0DnOa3nQrs8K/gdjFdByAijYFhqprvbjcF3gUeVNUlGGOMAZzuuOrqfvLxsmWxHOgsIh1FpCEwHJjnX0FEWouIL4b7ca6Mwq3/Ns7g9xsexmhMvWFTVtRvVf39e5YsVLUIGAd8AGwAZqvqOhGZKCJXudUGAhtFZBNwMvCYu/+/gAHArSKy0n2UvQvHGBORpKQk8vLyLGHUU6pKXl4eSUlJlT6GrcFtTD1w/PhxcnJySl3nb+qXpKQkUlNTSUhIKLXf1uA2xpRISEigY8eONR2GqcNsIkFjjDFhWbIwxhgTliULY4wxYcXMALeI5AJVuYW7NbAnSuHUFfXtnOvb+YKdc31RlXPuoKph72qOmWRRVSKyIpIrAmJJfTvn+na+YOdcX1THOVs3lDHGmLAsWRhjjAnLksVPwq+sEnvq2znXt/MFO+f6wvNztjELY4wxYVnLwhhjTFiWLIwxxoRV75OFiAwWkY0iki0i42s6nqoQkWkisltE1vrtaykiH4rIt+6/Ldz9IiLPuOe9WkR6+73mFrf+tyJyS02cS6RE5DQR+URENojIOhH5jbs/Zs9bRJJEZJmIrHLP+RF3f0cRWerG/7o71T8ikuhuZ7vlaX7Hut/dv1FEBtXMGUVGROJE5GsR+be7Hevnu1VE1rizbq9w99Xc51pV6+0DiAO+AzoBDYFVQJeajqsK5zMA6A2s9dv3F2C8+3w88Gf3+eXAfJwVDc/DWd4WoCWw2f23hfu8RU2fWznnfArQ233eBNiEs7Z7zJ63G3tj93kCsNQ9l9nAcHf/C8AY9/ndwAvu8+HA6+7zLu5nPhHo6P5fiKvp8yvnvH8HzAD+7W7H+vluxVlm2n9fjX2u63vLoi+QraqbVbUQmAVcXcMxVZqqLgb2Buy+GmcNc9x/r/Hb/6o6lgDNxVnGdhDwoaruVdV9wIfAYO+jrxxV3amqX7nPD+KsndKOGD5vN/ZD7maC+1DgEmCOuz/wnH0/iznAz0RE3P2zVPWYqm4BsnH+T9Q6IpIKXAG85G4LMXy+5aixz3V9TxbtgO1+2znuvlhysqruBOcPK3CSuz/UudfZn4nb3dAL55t2TJ+32yWzEtiN8wfgO2C/OouOQen4S87NLc8HWlG3zvnvwB+AYne7FbF9vuB8AVggIl+KyCh3X419ruv7ehbBVjqvL9cShzr3OvkzEWcN9zeBe1T1gIRexD4mzltVTwDpItIcZwnic4JVc/+t0+csIkOB3ar6pYgM9O0OUjUmztfPBaq6Q0ROAj4UkW/Kqev5Odf3lkUOcJrfdiqwo4Zi8cqPbnMU99/d7v5Q517nfiYikoCTKF5T1bfc3TF/3gCquh9YhNNP3VxEfF8A/eMvOTe3vBlOd2VdOecLgKtEZCtOV/ElOC2NWD1fAFR1h/vvbpwvBH2pwc91fU8Wy4HO7lUVDXEGw+bVcEzRNg/wXQFxC/Avv/0j3asozgPy3WbtB8BlItLCvdLiMndfreT2RU8FNqjq3/yKYva8RaSN26JARBoBl+KM1XwCXO9WCzxn38/ieuBjdUY/5wHD3auHOgKdgWXVcxaRU9X7VTVVVdNw/o9+rKo3EaPnCyAiKSLSxPcc5/O4lpr8XNf0iH9NP3CuItiE0+f7QE3HU8VzmQnsBI7jfKO4HaevdiHwrftvS7euAM+5570GyPA7zq9wBv+ygdtq+rzCnPOFOM3q1cBK93F5LJ830AP42j3ntcBD7v5OOH/8soE3gER3f5K7ne2Wd/I71gPuz2IjMKSmzy2Ccx/IT1dDxez5uue2yn2s8/1tqsnPtU33YYwxJqz63g1ljDEmApYsjDHGhGXJwhhjTFiWLIwxxoRlycIYY0xYlixMvSUiJ4vIDBHZ7E6p8B8RubaGYhkoIuf7bY8WkZE1EYsxwdT36T5MPeXezDcXeEVVf+nu6wBc5eF7xutPcxkFGggcAr4AUNUXvIrDmMqw+yxMvSQiP8O5mS0zSFkcMAnnD3gi8JyqvujOS/QwsAfoBnwJjFBVFZFzgb8Bjd3yW1V1p4gswkkAF+DcZbsJeBBnSvw84CagEbAEOAHkAr8GfgYcUtUnRSQdZwruZJybrn6lqvvcYy8FLgaaA7er6qfR+ykZ8xPrhjL1VVfgqxBlt+NMl9AH6APc6U4PAc6stvfgrI3QCbjAnZvqWeB6VT0XmAY85ne85qqaqapPAZ8B56lqL5x5jv6gqltxksH/U9X0IH/wXwXuU9UeOHfnTvAri1fVvm5MEzDGI9YNZQwgIs/hTB1SCGwDeoiIb96hZjjzCBUCy1Q1x33NSiAN2I/T0vjQne02DmfaFZ/X/Z6nAq+7k8A1BLaEiasZTrLJcne9gjOVhY9v4sQv3ViM8YQlC1NfrQOG+TZUdayItAZWAN8Dv1bVUhOuud1Qx/x2ncD5PyTAOlXtH+K9CvyePwv8TVXn+XVrVYUvHl8sxnjCuqFMffUxkCQiY/z2Jbv/fgCMcbuXEJEz3Zk/Q9kItBGR/m79BBHpGqJuM+AH97n/esgHcZaFLUVV84F9InKRu+tmICuwnjFes28ipl5yB6WvAf6fiPwBZ2C5ALgPp5snDfjKvWoql5+Wrwx2rEK3y+oZt9soHme9hXVBqj8MvCEiP+AMavvGQt4B5ojI1TgD3P5uAV4QkWScNZRvq/gZG1M1djWUMcaYsKwbyhhjTFiWLIwxxoRlycIYY0xYliyMMcaEZcnCGGNMWJYsjDHGhGXJwhhjTFj/Hx+j6EvehbJZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first plot: Plot the loss and accuracies of training and test data\n",
    "eval_indices = range(0, generations+eval_every, 100)\n",
    "plt.plot(eval_indices, train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "# second plot: Plot train and test accuracy\n",
    "plt.plot(eval_indices, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(eval_indices, test_acc, 'r--', label='Test Set Accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: neural_network_model_only_cost_and_meter\\saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'neural_network_model_only_cost_and_meter\\\\saved_model.pb'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following builder can be used for creating servable models that can be used for production\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(\"neural_network_model_only_cost_and_meter\")\n",
    "builder.add_meta_graph_and_variables(sess,[tf.saved_model.tag_constants.SERVING], {\n",
    "            \"mietkartei\": tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "                inputs= {\"features\": X, \"prob_keep\": prob_keep},\n",
    "                outputs= {\"label_index\": Y_one_hot, \"propability\": Y})\n",
    "            })\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: checkpoints/_only_cost_and_meter/neural_network_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# the following saver stores the all variable values (weights and biases) of the trained model for restoring the model anytime\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"checkpoints/only_cost_and_meter/neural_network_model.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
